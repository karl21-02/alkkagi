"""
=================================================================
ì•Œê¹Œê¸° ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ (PPO + Self-Play)
=================================================================

ì•Œê³ ë¦¬ì¦˜: PPO (Proximal Policy Optimization)
í•™ìŠµ ë°©ì‹: Self-Play (ê³¼ê±° ìì‹ ê³¼ ëŒ€ê²°í•˜ë©° í•™ìŠµ)

í•µì‹¬ ê°œì„ ì‚¬í•­:
1. [FIX] ëŒ êµ¬ë¶„ ë¬¸ì œ í•´ê²°: stone_id ì¶”ê°€ë¡œ 1ë²ˆ/2ë²ˆ/3ë²ˆ ëŒ ë…ë¦½ í•™ìŠµ
2. [FIX] Critic loss ê°€ì¤‘ì¹˜ ì¦ê°€: 0.5 â†’ 1.0 (ê°€ì¹˜ í•¨ìˆ˜ ë¹ ë¥¸ í•™ìŠµ)
3. [FIX] GPU ìë™ ìµœì í™”: GPU ìˆìœ¼ë©´ 32 envs, ì—†ìœ¼ë©´ 8 envs
4. [IMPROVED] ë„¤íŠ¸ì›Œí¬: Shared trunk + ReLU + LayerNorm + ì¶œë ¥ Tanh ì œê±°
5. [IMPROVED] ê´€ì¸¡: 54ì°¨ì› (stone_id, ê²½ê³„ ê±°ë¦¬, ëª¨ë“  ì  ê±°ë¦¬, ê²Œì„ í˜ì´ì¦ˆ)
6. [IMPROVED] ë³´ìƒ: ëª…ì¤‘ í™•ë¥ , ê³¨ ê·¼ì ‘ë„, ê· í˜•ì¡íŒ kill/suicide
7. [IMPROVED] í•™ìŠµë¥ /ì—”íŠ¸ë¡œí”¼ ìŠ¤ì¼€ì¤„ë§: ì´ˆë°˜ íƒí—˜ â†’ í›„ë°˜ ì°©ì·¨

êµ¬ì¡°:
- ActorCritic: ê³µìœ  ë„¤íŠ¸ì›Œí¬ + Actor/Critic heads
- BaseAgent: ê´€ì¸¡ ì²˜ë¦¬, í–‰ë™ ë””ì½”ë”©, PPO í•™ìŠµ
- OpponentManager: Self-play ìƒëŒ€ í’€ ê´€ë¦¬
- train(): ë©”ì¸ í•™ìŠµ ë£¨í”„

=================================================================
"""

import gymnasium as gym
import kymnasium as kym
import numpy as np
import os
import glob
import random
import shutil
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Normal
from typing import Any, Dict
from gymnasium.vector import AsyncVectorEnv

# ==========================================
# 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (ìµœì í™”ë¨!)
# ==========================================

# GPU ê°€ì† ì„¤ì •
# - GPUê°€ ìˆìœ¼ë©´ CUDA ì‚¬ìš©, ì—†ìœ¼ë©´ CPU ì‚¬ìš©
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
USE_GPU_OPTIMIZED = torch.cuda.is_available()

# í™˜ê²½ ë° ë°°ì¹˜ ì„¤ì • (GPU ìœ ë¬´ì— ë”°ë¼ ìë™ ì¡°ì •)
if USE_GPU_OPTIMIZED:
    NUM_ENVS = 32      # ë³‘ë ¬ë¡œ ì‹¤í–‰í•  ê²Œì„ í™˜ê²½ ê°œìˆ˜ (GPUë©´ 32ê°œ)
    BATCH_SIZE = 512   # í•œ ë²ˆì— í•™ìŠµí•  ë°ì´í„° ê°œìˆ˜ (GPUë©´ 512ê°œ)
    T_HORIZON = 512    # í•œ ë²ˆì— ìˆ˜ì§‘í•  ê²½í—˜ ìŠ¤í… ìˆ˜ (GPUë©´ 512)
else:
    NUM_ENVS = 8       # CPUëŠ” 8ê°œë¡œ ì¤„ì„ (ë©”ëª¨ë¦¬ ì ˆì•½)
    BATCH_SIZE = 256   # CPUëŠ” 256ê°œ
    T_HORIZON = 256    # CPUëŠ” 256

# í•™ìŠµë¥  ì„¤ì • (ì‹œê°„ì´ ì§€ë‚˜ë©´ì„œ ì ì°¨ ê°ì†Œ)
LR_ACTOR_START = 0.0003   # Actor ì´ˆê¸° í•™ìŠµë¥  (ì •ì±… í•™ìŠµ)
LR_ACTOR_END = 0.00003    # Actor ìµœì¢… í•™ìŠµë¥  (í•™ìŠµ í›„ë°˜)
LR_CRITIC = 0.0005        # Critic í•™ìŠµë¥  (ê°€ì¹˜ í•¨ìˆ˜ í•™ìŠµ)

# PPO ì•Œê³ ë¦¬ì¦˜ ì„¤ì •
GAMMA = 0.96         # í• ì¸ìœ¨: ë¯¸ë˜ ë³´ìƒì„ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•˜ê²Œ ë³¼ì§€ (0.96 = 96%)
K_EPOCHS = 10        # ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ëª‡ ë²ˆ ì¬ì‚¬ìš©í• ì§€
EPS_CLIP = 0.2       # PPO clipping ë²”ìœ„ (ì •ì±… ë³€í™”ë¥¼ ì œí•œ)
MAX_GRAD_NORM = 0.5  # Gradient clipping (í•™ìŠµ ì•ˆì •í™”)

# íƒí—˜ ê³„ìˆ˜ (ì‹œê°„ì´ ì§€ë‚˜ë©´ì„œ íƒí—˜ ê°ì†Œ â†’ ì°©ì·¨ ì¦ê°€)
ENTROPY_COEF_START = 0.15  # ì´ˆë°˜: ëœë¤í•˜ê²Œ ë§ì´ íƒí—˜
ENTROPY_COEF_END = 0.01    # í›„ë°˜: í•™ìŠµí•œ ì •ì±…ëŒ€ë¡œ í”Œë ˆì´

# Self-Play ì„¤ì •
SELFPLAY_SAVE_INTERVAL = 50  # 50 ì—…ë°ì´íŠ¸ë§ˆë‹¤ ëª¨ë¸ ì €ì¥
SELFPLAY_SWAP_INTERVAL = 20  # 20 ì—…ë°ì´íŠ¸ë§ˆë‹¤ ìƒëŒ€ êµì²´

# ì»¤ë¦¬í˜ëŸ¼ ëŸ¬ë‹ ì„¤ì • (ì‰¬ìš´ ìƒëŒ€ â†’ ì–´ë ¤ìš´ ìƒëŒ€)
USE_CURRICULUM = True                   # ì»¤ë¦¬í˜ëŸ¼ ì‚¬ìš© ì—¬ë¶€
CURRICULUM_THRESHOLD = 0.15             # 15% ìŠ¹ë¥  ë„˜ìœ¼ë©´ ë‚œì´ë„ ì¦ê°€
RANDOM_OPPONENT_PROB_START = 0.8        # ì²˜ìŒì—” 80% ëœë¤ ìƒëŒ€
RANDOM_OPPONENT_PROB_MIN = 0.1          # ë‚˜ì¤‘ì—” 10% ëœë¤ ìƒëŒ€


# ==========================================
# 2. ì‹ ê²½ë§ êµ¬ì¡° (Actor-Critic)
# ==========================================
class ActorCritic(nn.Module):
    """
    Actor-Critic ë„¤íŠ¸ì›Œí¬
    - Actor: í–‰ë™(ì–´ë–»ê²Œ ëŒì„ ì ì§€)ì„ ê²°ì •
    - Critic: í˜„ì¬ ìƒí™©ì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ í‰ê°€
    - Shared trunk: Actorì™€ Criticì´ ì´ˆë°˜ ë ˆì´ì–´ë¥¼ ê³µìœ  (íš¨ìœ¨ì  í•™ìŠµ)
    """
    def __init__(self, state_dim, action_dim):
        super(ActorCritic, self).__init__()
        self.data = []  # ê²½í—˜ ë°ì´í„° ì €ì¥ ë²„í¼

        # ===== ê³µìœ  íŠ¹ì§• ì¶”ì¶œê¸° =====
        # Actorì™€ Criticì´ ê³µìœ í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ (íš¨ìœ¨ì !)
        # ì…ë ¥: ê²Œì„ ìƒíƒœ (54ì°¨ì›) â†’ ì¶œë ¥: íŠ¹ì§• ë²¡í„° (256ì°¨ì›)
        self.shared = nn.Sequential(
            nn.Linear(state_dim, 512),  # 54 â†’ 512
            nn.LayerNorm(512),           # í•™ìŠµ ì•ˆì •í™”
            nn.ReLU(),                   # í™œì„±í™” í•¨ìˆ˜ (ê¸°ìš¸ê¸° ì†Œì‹¤ ë°©ì§€)
            nn.Linear(512, 512),         # 512 â†’ 512 (ê¹Šì€ í•™ìŠµ)
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Linear(512, 256),         # 512 â†’ 256
            nn.ReLU()
        )

        # ===== Actor Head (ì •ì±…) =====
        # í–‰ë™ì˜ í‰ê· ê°’(mu) ì¶œë ¥
        # ì¤‘ìš”: ì¶œë ¥ì¸µì— Tanh ì—†ìŒ! (í–‰ë™ ë²”ìœ„ ì œí•œ X)
        self.actor_head = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)  # 4ì°¨ì› (ëŒ ì„ íƒ, íŒŒì›Œ, ë°©í–¥x, ë°©í–¥y)
        )

        # ===== í‘œì¤€í¸ì°¨ Head =====
        # ìƒí™©ì— ë”°ë¼ íƒí—˜ ì •ë„ë¥¼ ì¡°ì ˆ (ì¤‘ìš”í•œ ìˆœê°„ì—” ì‹ ì¤‘í•˜ê²Œ!)
        self.log_std_head = nn.Sequential(
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim)
        )

        # ===== Critic Head (ê°€ì¹˜ í•¨ìˆ˜) =====
        # í˜„ì¬ ìƒíƒœì˜ ê°€ì¹˜ë¥¼ í‰ê°€ (ì–¼ë§ˆë‚˜ ìœ ë¦¬í•œì§€)
        self.critic_head = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1)  # ë‹¨ì¼ ê°’ ì¶œë ¥ (ìƒíƒœ ê°€ì¹˜)
        )

    def pi(self, x):
        """
        ì •ì±… í•¨ìˆ˜: ì£¼ì–´ì§„ ìƒíƒœì—ì„œ í–‰ë™ ë¶„í¬ ë°˜í™˜
        - ì…ë ¥: ìƒíƒœ (54ì°¨ì›)
        - ì¶œë ¥: ì •ê·œë¶„í¬ (í‰ê· =mu, í‘œì¤€í¸ì°¨=std)
        """
        shared = self.shared(x)                    # ê³µìœ  íŠ¹ì§• ì¶”ì¶œ
        mu = self.actor_head(shared)               # í–‰ë™ í‰ê· 
        log_std = self.log_std_head(shared)        # í‘œì¤€í¸ì°¨ (log scale)
        log_std = torch.clamp(log_std, -20, 2)     # ìˆ˜ì¹˜ ì•ˆì •ì„± (ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ì§€ ì•Šê²Œ)
        std = log_std.exp()                        # ì‹¤ì œ í‘œì¤€í¸ì°¨
        dist = Normal(mu, std)                     # ì •ê·œë¶„í¬ ìƒì„±
        return dist

    def v(self, x):
        """
        ê°€ì¹˜ í•¨ìˆ˜: ì£¼ì–´ì§„ ìƒíƒœì˜ ê°€ì¹˜ í‰ê°€
        - ì…ë ¥: ìƒíƒœ (54ì°¨ì›)
        - ì¶œë ¥: ê°€ì¹˜ (ë‹¨ì¼ ì‹¤ìˆ˜)
        """
        shared = self.shared(x)
        return self.critic_head(shared)

    def put_data(self, transition):
        self.data.append(transition)

    def make_batch(self):
        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []
        for transition in self.data:
            s, a, r, s_prime, prob_a, done = transition
            s_lst.append(s)
            a_lst.append(a)
            r_lst.append(r)
            s_prime_lst.append(s_prime)
            prob_a_lst.append(prob_a)
            done_lst.append(done)
        self.data = []

        def to_tensor(d):
            return torch.tensor(np.array(d), dtype=torch.float).to(DEVICE).view(-1, d[0].shape[-1])

        def to_tensor_s(d):
            return torch.tensor(np.array(d), dtype=torch.float).to(DEVICE).view(-1, 1)

        return (to_tensor(s_lst), to_tensor(a_lst), to_tensor_s(r_lst),
                to_tensor(s_prime_lst), to_tensor_s(prob_a_lst), to_tensor_s(done_lst))


# ==========================================
# 3. Random Agent (ì»¤ë¦¬í˜ëŸ¼ ëŸ¬ë‹ìš©)
# ==========================================
class RandomAgent:
    """ì™„ì „ ëœë¤ í–‰ë™ì„ í•˜ëŠ” ì•½í•œ ìƒëŒ€"""

    def __init__(self):
        self.my_turn = 1

    def get_action_batch(self, obs_np):
        batch_size = obs_np.shape[0]
        # ëœë¤ í–‰ë™ ìƒì„±
        actions = np.random.uniform(-1, 1, (batch_size, 4)).astype(np.float32)
        log_probs = np.zeros(batch_size, dtype=np.float32)
        return actions, log_probs


# ==========================================
# 4. Base Agent Logic
# ==========================================
class BaseAgent(kym.Agent):
    def __init__(self, my_turn):
        super().__init__()
        self.my_turn = my_turn
        self.state_dim = 54  # [FIX] 51 -> 54 (stone_id 3ì°¨ì› ì¶”ê°€!)
        self.action_dim = 4
        self.model = ActorCritic(self.state_dim, self.action_dim).to(DEVICE)
        self.optimizer = optim.Adam(self.model.parameters(), lr=LR_ACTOR_START)  # ì‹œì‘ í•™ìŠµë¥  ì‚¬ìš©
        self.current_entropy_coef = ENTROPY_COEF_START

    def _process_batch_obs(self, obs, override_turn=None, selected_stone_idx=None):
        """
        ê²Œì„ ê´€ì¸¡ê°’ì„ ì‹ ê²½ë§ ì…ë ¥ìœ¼ë¡œ ë³€í™˜

        í•µì‹¬ ê°œì„ ì‚¬í•­:
        1. stone_id ì¶”ê°€: 1ë²ˆ ëŒê³¼ 2ë²ˆ ëŒì„ êµ¬ë¶„! (ì´ì „ì—ëŠ” ëª¨ë‘ ê°™ì€ í–‰ë™)
        2. ê²½ê³„ ê±°ë¦¬: ê³¨ë¼ì¸ê¹Œì§€ ê±°ë¦¬ ì¶”ê°€
        3. ëª¨ë“  ì ê³¼ì˜ ê±°ë¦¬: ì „ëµì  íƒ€ê²ŸíŒ… ê°€ëŠ¥
        4. ê²Œì„ í˜ì´ì¦ˆ ì •ë³´: ëŒ ê°œìˆ˜ ì°¨ì´, ì§ˆëŸ‰ ì¤‘ì‹¬

        Args:
            obs: ê²Œì„ ê´€ì¸¡ê°’ (black, white ëŒ ì •ë³´)
            override_turn: ê°•ì œë¡œ ì°¨ë¡€ ì§€ì • (0=í‘, 1=ë°±)
            selected_stone_idx: ì„ íƒëœ ëŒì˜ ì¸ë±ìŠ¤ (ì¤‘ìš”!)

        Returns:
            54ì°¨ì› íŠ¹ì§• ë²¡í„°
        """
        board_scale = 1000.0  # ë³´ë“œ ìŠ¤ì¼€ì¼ (ì¢Œí‘œ ì •ê·œí™”ìš©)
        batch_size = len(obs['black'])

        # í˜„ì¬ ì°¨ë¡€ ê²°ì • (í‘=0, ë°±=1)
        if override_turn is not None:
            turns = np.full((batch_size, 1, 1), override_turn)
        else:
            turns = obs['turn'].reshape(batch_size, 1, 1)

        black_stones = obs['black']  # í‘ëŒ ì •ë³´ [batch, 3, 3] (x, y, alive)
        white_stones = obs['white']  # ë°±ëŒ ì •ë³´ [batch, 3, 3]

        # ë‚´ ëŒ vs ìƒëŒ€ ëŒ êµ¬ë¶„
        my_stones = np.where(turns == 0, black_stones, white_stones)
        op_stones = np.where(turns == 0, white_stones, black_stones)

        # ì¢Œí‘œ ì •ê·œí™” ([-500, 500] â†’ [-0.5, 0.5])
        my_norm = np.copy(my_stones)
        my_norm[:, :, 0:2] /= board_scale
        op_norm = np.copy(op_stones)
        op_norm[:, :, 0:2] /= board_scale

        # í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
        my_xy = my_stones[:, :, 0:2]  # [B, 3, 2] ë‚´ ëŒ ì¢Œí‘œ
        op_xy = op_stones[:, :, 0:2]  # [B, 3, 2] ìƒëŒ€ ëŒ ì¢Œí‘œ
        my_alive = my_stones[:, :, 2]  # [B, 3] ë‚´ ëŒ ìƒì¡´ ì—¬ë¶€
        op_alive = op_stones[:, :, 2]  # [B, 3] ìƒëŒ€ ëŒ ìƒì¡´ ì—¬ë¶€

        # ===== [í•µì‹¬ FIX!] ëŒ êµ¬ë¶„ ID ì¶”ê°€ =====
        # ë¬¸ì œ: 1ë²ˆ ëŒì´ ì£½ìœ¼ë©´ 2ë²ˆ ëŒë„ ë˜‘ê°™ì´ í–‰ë™í•¨
        # í•´ê²°: ê° ëŒì— ê³ ìœ  ID ë¶€ì—¬ (one-hot encoding)
        if selected_stone_idx is not None:
            # ì„ íƒëœ ëŒë§Œ 1, ë‚˜ë¨¸ì§€ëŠ” 0
            # ì˜ˆ: 1ë²ˆ ëŒ ì„ íƒ â†’ [0, 1, 0]
            stone_id = np.zeros((batch_size, 3), dtype=np.float32)
            stone_id[np.arange(batch_size), selected_stone_idx] = 1.0
        else:
            # ì„ íƒ ì•ˆ ëìœ¼ë©´ ê· ë“± ë¶„í¬
            stone_id = np.ones((batch_size, 3), dtype=np.float32) / 3.0

        # ===== ê°€ì¥ ê°€ê¹Œìš´ ì  ì •ë³´ ê³„ì‚° =====
        # ê° ë‚´ ëŒë§ˆë‹¤ ê°€ì¥ ê°€ê¹Œìš´ ì ì„ ì°¾ìŒ
        diff = op_xy[:, np.newaxis, :, :] - my_xy[:, :, np.newaxis, :]  # ëª¨ë“  ìŒì˜ ì°¨ì´ë²¡í„°
        dist_sq = np.sum(diff ** 2, axis=-1)  # ê±°ë¦¬ì˜ ì œê³±
        mask = (1 - op_alive[:, np.newaxis, :]) * 1e9  # ì£½ì€ ì ì€ ë§¤ìš° ë¨¼ ê²ƒìœ¼ë¡œ ì²˜ë¦¬
        dist_sq += mask
        min_idx = np.argmin(dist_sq, axis=2)  # ê° ë‚´ ëŒë§ˆë‹¤ ê°€ì¥ ê°€ê¹Œìš´ ì ì˜ ì¸ë±ìŠ¤

        batch_idx = np.arange(batch_size)[:, np.newaxis]
        my_idx = np.arange(3)[np.newaxis, :]
        target_diff = diff[batch_idx, my_idx, min_idx, :]  # ê°€ì¥ ê°€ê¹Œìš´ ì ìœ¼ë¡œì˜ ë²¡í„°

        # ê±°ë¦¬ì™€ ë°©í–¥ ê³„ì‚°
        raw_dist = np.sqrt(np.sum(target_diff ** 2, axis=-1))
        safe_dist = raw_dist + 1e-6  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        target_u_x = target_diff[:, :, 0] / safe_dist  # ë‹¨ìœ„ ë²¡í„° x
        target_u_y = target_diff[:, :, 1] / safe_dist  # ë‹¨ìœ„ ë²¡í„° y
        target_dist_norm = raw_dist / board_scale  # ì •ê·œí™”ëœ ê±°ë¦¬

        # ===== [ì‹ ê·œ] ë³´ë“œ ê²½ê³„ê¹Œì§€ì˜ ê±°ë¦¬ =====
        # ê³¨ë¼ì¸ì´ë‚˜ ë²½ê¹Œì§€ ê±°ë¦¬ëŠ” ì „ëµì ìœ¼ë¡œ ì¤‘ìš”!
        # ë³´ë“œ ë²”ìœ„: [-500, 500] Ã— [-500, 500]
        dist_to_left = (my_xy[:, :, 0] + 500) / board_scale
        dist_to_right = (500 - my_xy[:, :, 0]) / board_scale
        dist_to_top = (my_xy[:, :, 1] + 500) / board_scale
        dist_to_bottom = (500 - my_xy[:, :, 1]) / board_scale
        boundary_dists = np.stack([dist_to_left, dist_to_right, dist_to_top, dist_to_bottom], axis=-1)

        # ===== [ì‹ ê·œ] ëª¨ë“  ì ê³¼ì˜ ê±°ë¦¬ =====
        # ê°€ì¥ ê°€ê¹Œìš´ ì ë§Œì´ ì•„ë‹ˆë¼ ëª¨ë“  ì ê³¼ì˜ ê±°ë¦¬
        # â†’ ì „ëµì  íƒ€ê²ŸíŒ… ê°€ëŠ¥ (ìœ„í˜‘ì ì¸ ì ë¶€í„° ì œê±°)
        diff_all = my_xy[:, :, np.newaxis, :] - op_xy[:, np.newaxis, :, :]  # [B, 3, 3, 2]
        dist_all = np.linalg.norm(diff_all, axis=-1) / board_scale  # [B, 3, 3]
        dist_all += (1 - op_alive[:, np.newaxis, :]) * 10.0  # ì£½ì€ ëŒ ë§ˆìŠ¤í‚¹

        # ===== [ì‹ ê·œ] ê²Œì„ í˜ì´ì¦ˆ ì •ë³´ =====
        # ëŒ ê°œìˆ˜ ì°¨ì´: ë‚´ê°€ ìœ ë¦¬í•œì§€ ë¶ˆë¦¬í•œì§€
        my_count = np.sum(my_alive, axis=1, keepdims=True)
        op_count = np.sum(op_alive, axis=1, keepdims=True)
        count_diff = (my_count - op_count) / 3.0  # [-1, 1] ë²”ìœ„

        # ì§ˆëŸ‰ ì¤‘ì‹¬ ì°¨ì´: ê³µê²©/ìˆ˜ë¹„ ìœ„ì¹˜ íŒŒì•…
        my_center = np.sum(my_xy * my_alive[:, :, np.newaxis], axis=1) / (my_count + 1e-6)
        op_center = np.sum(op_xy * op_alive[:, :, np.newaxis], axis=1) / (op_count + 1e-6)
        center_diff = (my_center - op_center) / board_scale

        # ===== ëª¨ë“  íŠ¹ì§• ê²°í•© (ì´ 54ì°¨ì›) =====
        flat_obs = np.concatenate([
            stone_id,                                           # 3ì°¨ì› - ëŒ ID (í•µì‹¬!)
            my_norm.reshape(batch_size, -1),                    # 9ì°¨ì› - ë‚´ ëŒ ì •ë³´
            op_norm.reshape(batch_size, -1),                    # 9ì°¨ì› - ìƒëŒ€ ëŒ ì •ë³´
            target_dist_norm[:, :, np.newaxis].reshape(batch_size, -1),  # 3ì°¨ì› - ê°€ê¹Œìš´ ì  ê±°ë¦¬
            target_u_y[:, :, np.newaxis].reshape(batch_size, -1),        # 3ì°¨ì› - ê°€ê¹Œìš´ ì  ë°©í–¥Y
            target_u_x[:, :, np.newaxis].reshape(batch_size, -1),        # 3ì°¨ì› - ê°€ê¹Œìš´ ì  ë°©í–¥X
            boundary_dists.reshape(batch_size, -1),             # 12ì°¨ì› - ê²½ê³„ ê±°ë¦¬
            dist_all.reshape(batch_size, -1),                   # 9ì°¨ì› - ëª¨ë“  ì  ê±°ë¦¬
            count_diff,                                          # 1ì°¨ì› - ëŒ ê°œìˆ˜ ì°¨ì´
            center_diff                                          # 2ì°¨ì› - ì§ˆëŸ‰ì¤‘ì‹¬ ì°¨ì´
        ], axis=1)

        return flat_obs.astype(np.float32)  # ì´ 54ì°¨ì›!

    def _process_single_obs(self, obs):
        batch_obs = {'black': np.array([obs['black']]), 'white': np.array([obs['white']]), 'turn': np.array([0])}
        return self._process_batch_obs(batch_obs, override_turn=self.my_turn)[0]

    def _decode_action(self, action_tensor):
        if isinstance(action_tensor, torch.Tensor):
            a = action_tensor.cpu().numpy().flatten()
        else:
            a = action_tensor.flatten()

        raw_idx = (a[0] + 1) / 2.0
        idx = int(np.clip(raw_idx * 3, 0, 2))

        power = float(300.0 + ((a[1] + 1) / 2.0) * 2200.0)

        dx = a[2]
        dy = a[3]
        angle = float(np.degrees(np.arctan2(dy, dx)))

        return {"turn": self.my_turn, "index": idx, "power": power, "angle": angle}

    def decode_batch_action(self, action_np, current_turns):
        raw_idx = (action_np[:, 0] + 1) / 2.0
        idx = np.clip(raw_idx * 3, 0, 2).astype(np.int32)
        power = 300.0 + ((action_np[:, 1] + 1) / 2.0) * 2200.0

        dx = action_np[:, 2]
        dy = action_np[:, 3]
        angle = np.degrees(np.arctan2(dy, dx))

        return {"turn": current_turns.astype(np.int32), "index": idx, "power": power.astype(np.float32),
                "angle": angle.astype(np.float32)}

    def get_action_batch(self, obs_np):
        obs_tensor = torch.tensor(obs_np, dtype=torch.float).to(DEVICE)
        dist = self.model.pi(obs_tensor)
        action = dist.sample()
        log_prob = dist.log_prob(action).sum(-1).detach().cpu().numpy()
        return torch.tanh(action).cpu().numpy(), log_prob

    def act(self, observation, info):
        obs_np = self._process_single_obs(observation)
        obs_tensor = torch.tensor(obs_np, dtype=torch.float).to(DEVICE)
        with torch.no_grad():
            shared = self.model.shared(obs_tensor)
            mu = self.model.actor_head(shared)
        return self._decode_action(mu)

    def train_net(self):
        if len(self.model.data) < 1: return
        s, a, r, s_prime, prob_a, done_mask = self.model.make_batch()
        with torch.no_grad():
            td_target = r + GAMMA * self.model.v(s_prime) * (1 - done_mask)
            delta = td_target - self.model.v(s)
        advantage = delta.detach()
        total_samples = s.size(0)
        indices = np.arange(total_samples)
        for _ in range(K_EPOCHS):
            np.random.shuffle(indices)
            for start in range(0, total_samples, BATCH_SIZE):
                idx = indices[start:start + BATCH_SIZE]
                dist = self.model.pi(s[idx])
                cur_log_prob = dist.log_prob(a[idx]).sum(-1).unsqueeze(1)
                ratio = torch.exp(cur_log_prob - prob_a[idx])
                surr1 = ratio * advantage[idx]
                surr2 = torch.clamp(ratio, 1 - EPS_CLIP, 1 + EPS_CLIP) * advantage[idx]
                loss = -torch.min(surr1, surr2).mean() + 1.0 * F.smooth_l1_loss(self.model.v(s[idx]), td_target[
                    idx]) - self.current_entropy_coef * dist.entropy().sum(-1).mean()  # [FIX] 0.5 -> 1.0 (Critic ê°•í™”!)
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), MAX_GRAD_NORM)  # [IMPROVED] ìƒìˆ˜ ì‚¬ìš©
                self.optimizer.step()


# ==========================================
# 5. Wrappers
# ==========================================
class YourBlackAgent(BaseAgent):
    def __init__(self):
        super().__init__(my_turn=0)

    @classmethod
    def load(cls, path):
        agent = cls()
        if os.path.exists(path):
            agent.model.load_state_dict(torch.load(path, map_location=DEVICE))
        elif os.path.exists(path + ".pkl"):
            agent.model.load_state_dict(torch.load(path + ".pkl", map_location=DEVICE))
        return agent

    def save(self, path):
        torch.save(self.model.state_dict(), path)


class YourWhiteAgent(BaseAgent):
    def __init__(self):
        super().__init__(my_turn=1)

    @classmethod
    def load(cls, path):
        agent = cls()
        if os.path.exists(path):
            agent.model.load_state_dict(torch.load(path, map_location=DEVICE))
        elif os.path.exists(path + ".pkl"):
            agent.model.load_state_dict(torch.load(path + ".pkl", map_location=DEVICE))
        return agent

    def save(self, path):
        torch.save(self.model.state_dict(), path)


# ==========================================
# 6. Manager & Loop
# ==========================================
class OpponentManager:
    def __init__(self):
        self.save_dir = "history_models"
        os.makedirs(self.save_dir, exist_ok=True)
        self.pool = glob.glob(os.path.join(self.save_dir, "model_*.pkl"))
        if not self.pool: self.save_current_model(YourBlackAgent().model, 0)

    def save_current_model(self, model, step):
        path = os.path.join(self.save_dir, f"model_{step}.pkl")
        torch.save(model.state_dict(), path)
        self.pool.append(path)
        if len(self.pool) > 20:
            old = self.pool.pop(0)
            if os.path.exists(old): os.remove(old)

    def get_opponent(self):
        path = random.choice(self.pool)
        op = YourBlackAgent()
        try:
            op.model.load_state_dict(torch.load(path, map_location=DEVICE)); op.model.eval(); return op
        except:
            return None


def make_env(): return gym.make(id='kymnasium/AlKkaGi-3x3-v0', render_mode=None, bgm=False, obs_type='custom')


def calculate_min_dist(black, white):
    """ê° í™˜ê²½ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ì ê³¼ì˜ ê±°ë¦¬ ê³„ì‚°"""
    b_xy = black[:, :, 0:2];
    b_alive = black[:, :, 2]
    w_xy = white[:, :, 0:2];
    w_alive = white[:, :, 2]
    diff = b_xy[:, :, np.newaxis, :] - w_xy[:, np.newaxis, :, :]
    dist = np.sqrt(np.sum(diff ** 2, axis=-1))
    mask = (1 - b_alive[:, :, np.newaxis] * w_alive[:, np.newaxis, :]) * 1e5
    dist += mask
    return np.min(dist, axis=(1, 2))


def train():
    print(f"ğŸš€ Starting OPTIMIZED Training!")
    print(f"ğŸ“Š Hyperparameters:")
    print(f"   - Learning Rate (Actor): {LR_ACTOR_START} -> {LR_ACTOR_END}")
    print(f"   - Learning Rate (Critic): {LR_CRITIC}")
    print(f"   - Entropy Coefficient: {ENTROPY_COEF_START} -> {ENTROPY_COEF_END}")
    print(f"   - GAMMA: {GAMMA}")
    print(f"   - K_EPOCHS: {K_EPOCHS}")
    print(f"   - NUM_ENVS: {NUM_ENVS}")
    print(f"   - Curriculum Learning: {USE_CURRICULUM}")

    envs = AsyncVectorEnv([make_env for _ in range(NUM_ENVS)])
    agent = YourBlackAgent()

    if os.path.exists("my_alkkagi_agent.pkl"):
        print("âœ… Found existing model. Resuming training...")
        try:
            ckpt = torch.load("my_alkkagi_agent.pkl", map_location=DEVICE)
            agent.model.load_state_dict(ckpt)
        except Exception as e:
            print(f"âš ï¸ Failed to load model: {e}. Starting fresh.")
    else:
        print("ğŸš€ Starting fresh training.")

    op_manager = OpponentManager()
    opponent = op_manager.get_opponent()
    random_agent = RandomAgent()  # [ì‹ ê·œ] ëœë¤ ìƒëŒ€

    obs, _ = envs.reset()
    prev_opp = np.sum(obs['white'][:, :, 2], axis=1)
    prev_my = np.sum(obs['black'][:, :, 2], axis=1)
    prev_min_dist = calculate_min_dist(obs['black'], obs['white'])  # [ì‹ ê·œ] ê±°ë¦¬ ì¶”ì 

    score_history = []
    interval_win_cnt = 0
    interval_total_cnt = 0
    recent_win_rates = []  # [ì‹ ê·œ] ì»¤ë¦¬í˜ëŸ¼ ì§„í–‰ì„ ìœ„í•œ ìŠ¹ë¥  ì¶”ì 

    for update in range(1, 10001):
        # [IMPROVED] í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
        progress = min(update / 10000.0, 1.0)
        current_lr = LR_ACTOR_START + (LR_ACTOR_END - LR_ACTOR_START) * progress
        for param_group in agent.optimizer.param_groups:
            param_group['lr'] = current_lr

        # [IMPROVED] ì—”íŠ¸ë¡œí”¼ ìŠ¤ì¼€ì¤„ë§ (ì´ˆë°˜ íƒí—˜ -> í›„ë°˜ ì°©ì·¨)
        agent.current_entropy_coef = ENTROPY_COEF_START + (ENTROPY_COEF_END - ENTROPY_COEF_START) * min(update / 5000.0, 1.0)

        if update % SELFPLAY_SAVE_INTERVAL == 0:
            op_manager.save_current_model(agent.model, update)
            print(f"ğŸ’¾ Saved model. Pool: {len(op_manager.pool)}")

        if update % SELFPLAY_SWAP_INTERVAL == 0:
            opponent = op_manager.get_opponent()
            print("ğŸ”„ Swapped opponent.")

        # [ì‹ ê·œ] ì»¤ë¦¬í˜ëŸ¼ ëŸ¬ë‹: ìŠ¹ë¥ ì— ë”°ë¼ ëœë¤ ìƒëŒ€ ë¹„ìœ¨ ì¡°ì •
        if USE_CURRICULUM and len(recent_win_rates) > 0:
            avg_recent_wr = np.mean(recent_win_rates[-10:])  # ìµœê·¼ 10ê°œ í‰ê· 
            random_prob = max(RANDOM_OPPONENT_PROB_MIN,
                              RANDOM_OPPONENT_PROB_START - (avg_recent_wr / CURRICULUM_THRESHOLD) * 0.7)
        else:
            random_prob = RANDOM_OPPONENT_PROB_START

        for _ in range(T_HORIZON):
            turns = obs['turn']
            actions_np = np.zeros((NUM_ENVS, 4), dtype=np.float32)
            log_probs_np = np.zeros(NUM_ENVS, dtype=np.float32)

            my_idx = np.where(turns == 0)[0]
            if len(my_idx) > 0:
                obs_me = agent._process_batch_obs(obs, override_turn=0)[my_idx]
                a, p = agent.get_action_batch(obs_me)
                actions_np[my_idx] = a;
                log_probs_np[my_idx] = p

            op_idx = np.where(turns == 1)[0]
            if len(op_idx) > 0:
                # [ì‹ ê·œ] í™•ë¥ ì ìœ¼ë¡œ ëœë¤ ìƒëŒ€ ë˜ëŠ” ì…€í”„í”Œë ˆì´ ìƒëŒ€ ì„ íƒ
                if random.random() < random_prob:
                    curr_op = random_agent
                else:
                    curr_op = opponent if opponent else agent

                obs_op = agent._process_batch_obs(obs, override_turn=1)[op_idx]
                with torch.no_grad():
                    a, _ = curr_op.get_action_batch(obs_op)
                actions_np[op_idx] = a

            real_actions = agent.decode_batch_action(actions_np, turns)
            next_obs, _, term, trunc, _ = envs.step(real_actions)

            curr_opp = np.sum(next_obs['white'][:, :, 2], axis=1)
            curr_my = np.sum(next_obs['black'][:, :, 2], axis=1)
            curr_min_dist = calculate_min_dist(next_obs['black'], next_obs['white'])  # [ì‹ ê·œ]

            if len(my_idx) > 0:
                # ===== ë³´ìƒ í•¨ìˆ˜ ê³„ì‚° (í•µì‹¬!) =====
                # ì¢‹ì€ ë³´ìƒ í•¨ìˆ˜ = ìŠ¹ë¦¬ë¡œ ì´ì–´ì§€ëŠ” í–‰ë™ì„ ëª…í™•íˆ ì•Œë ¤ì¤Œ

                # 1. ê¸°ë³¸ ë³´ìƒ: ì  ì œê±° vs ìì‚´
                kill = prev_opp[my_idx] - curr_opp[my_idx]      # ì  ëª‡ ê°œ ì£½ì˜€ë‚˜
                suicide = prev_my[my_idx] - curr_my[my_idx]    # ë‚´ ëŒ ëª‡ ê°œ ìƒì—ˆë‚˜
                r = (kill * 100.0) - (suicide * 100.0)         # [FIX] ê· í˜• ì¡°ì •! (ì´ì „: 200 vs 50)

                # 2. ê³¨ ê·¼ì ‘ë„ ë³´ìƒ (ì•Œê¹Œê¸°ëŠ” ê³¨ ê²Œì„!)
                # ì  ê³¨ë¼ì¸(x=500)ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ë¦¬
                b_xy_after = next_obs['black'][my_idx, :, 0:2]
                b_alive_after = next_obs['black'][my_idx, :, 2]
                avg_x_pos = np.sum(b_xy_after[:, :, 0] * b_alive_after, axis=1) / (np.sum(b_alive_after, axis=1) + 1e-6)
                goal_proximity = avg_x_pos / 1000.0  # [-0.5, 0.5]ë¡œ ì •ê·œí™”
                r += goal_proximity * 10.0  # ê³¨ ìª½ìœ¼ë¡œ ì´ë™í•˜ë©´ ë³´ìƒ!

                # 3. ëª…ì¤‘ í™•ë¥  ê¸°ë°˜ ì¡°ì¤€ ë³´ìƒ
                # ë‹¨ìˆœíˆ "ë°©í–¥"ë§Œì´ ì•„ë‹ˆë¼ "ëª…ì¤‘ ê°€ëŠ¥ì„±"ì„ í‰ê°€!
                agent_dx = actions_np[my_idx, 2]
                agent_dy = actions_np[my_idx, 3]
                agent_len = np.sqrt(agent_dx ** 2 + agent_dy ** 2) + 1e-6
                shot_dir = np.stack([agent_dx / agent_len, agent_dy / agent_len], axis=-1)  # ë°œì‚¬ ë°©í–¥ ë‹¨ìœ„ë²¡í„°

                b_xy = obs['black'][my_idx, :, 0:2]
                w_xy = obs['white'][my_idx, :, 0:2]
                w_alive = obs['white'][my_idx, :, 2]

                # ì„ íƒí•œ ëŒì˜ ìœ„ì¹˜
                raw_s_idx = (actions_np[my_idx, 0] + 1) / 2.0
                sel_idx = np.clip(raw_s_idx * 3, 0, 2).astype(int)
                k_rng = np.arange(len(my_idx))
                my_pos = b_xy[k_rng, sel_idx]

                # ê° ì ì— ëŒ€í•œ ëª…ì¤‘ ê°€ëŠ¥ì„± ê³„ì‚°
                diff = w_xy - my_pos[:, np.newaxis, :]  # ì ìœ¼ë¡œì˜ ë²¡í„°
                dist = np.linalg.norm(diff, axis=-1)  # ê±°ë¦¬
                diff_normalized = diff / (dist[:, :, np.newaxis] + 1e-6)  # ë°©í–¥

                # ì¡°ì¤€ ì •ë ¬ë„: ë°œì‚¬ ë°©í–¥ê³¼ ì  ë°©í–¥ì´ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ë‚˜
                alignment = np.sum(diff_normalized * shot_dir[:, np.newaxis, :], axis=-1)

                # ê±°ë¦¬ ê³„ìˆ˜: ê°€ê¹Œìš¸ìˆ˜ë¡ ëª…ì¤‘ ì‰¬ì›€
                dist_factor = np.exp(-dist / 500.0)

                # ìµœì¢… ëª…ì¤‘ ì ìˆ˜ = ì¡°ì¤€ Ã— ê±°ë¦¬ Ã— ìƒì¡´ì—¬ë¶€
                hit_score = alignment * dist_factor * w_alive
                best_hit_score = np.max(hit_score, axis=1)  # ê°€ì¥ ì˜ ì¡°ì¤€í•œ ì 
                r += best_hit_score * 5.0  # ì˜ ì¡°ì¤€í•˜ë©´ ë³´ìƒ!

                # 4. íŒŒì›Œ íš¨ìœ¨ì„±: ë„ˆë¬´ ì•½í•œ ìƒ·ì€ ë¬´ì˜ë¯¸
                power = 300.0 + ((actions_np[my_idx, 1] + 1) / 2.0) * 2200.0
                power_normalized = (power - 300.0) / 2200.0
                weak_shot_penalty = np.maximum(0, 0.3 - power_normalized) * 2.0
                r -= weak_shot_penalty  # ì•½í•œ ìƒ·ì€ í˜ë„í‹°

                # 5. ìŠ¹/íŒ¨ ë³´ìƒ (ê²Œì„ ì¢…ë£Œ ì‹œ)
                done = np.logical_or(term, trunc)[my_idx]
                win = (curr_opp[my_idx] == 0) & done   # ì  ì „ë©¸ = ìŠ¹ë¦¬
                lose = (curr_my[my_idx] == 0) & done   # ë‚´ ì „ë©¸ = íŒ¨ë°°

                r[win] += 200.0   # [FIX] ìŠ¹ë¦¬ ë³´ë„ˆìŠ¤ (ì´ì „: 500, ë„ˆë¬´ ì»¸ìŒ)
                r[lose] -= 200.0  # [FIX] íŒ¨ë°° í˜ë„í‹° (ì´ì „: -100, ë„ˆë¬´ ì•½í–ˆìŒ)

                for k, d in enumerate(done):
                    if d:
                        interval_total_cnt += 1
                        if win[k]: interval_win_cnt += 1

                s = agent._process_batch_obs(obs, override_turn=0)[my_idx]
                s_p = agent._process_batch_obs(next_obs, override_turn=0)[my_idx]
                for k, i in enumerate(my_idx):
                    agent.model.put_data(
                        (s[k], actions_np[i], r[k] / 100.0, s_p[k], log_probs_np[i], done.astype(float)[k]))
                    score_history.append(r[k])

            obs = next_obs
            prev_opp = curr_opp
            prev_my = curr_my
            prev_min_dist = curr_min_dist  # [ì‹ ê·œ] ê±°ë¦¬ ì—…ë°ì´íŠ¸

        agent.train_net()

        if update % 5 == 0 and score_history:
            avg_score = np.mean(score_history[-1000:]) * 100.0
            if interval_total_cnt > 0:
                win_rate = (interval_win_cnt / interval_total_cnt) * 100.0
                recent_win_rates.append(win_rate / 100.0)  # [ì‹ ê·œ] ìŠ¹ë¥  ê¸°ë¡
                if len(recent_win_rates) > 50:
                    recent_win_rates.pop(0)

                print(
                    f"Update: {update}, Avg Reward: {avg_score:.2f}, Win Rate: {win_rate:.1f}% ({interval_win_cnt}/{interval_total_cnt}), Random Opp: {random_prob * 100:.0f}%")
            else:
                print(
                    f"Update: {update}, Avg Reward: {avg_score:.2f}, Win Rate: 0.0% (0/0), Random Opp: {random_prob * 100:.0f}%")
            interval_win_cnt = 0
            interval_total_cnt = 0
            agent.save("my_alkkagi_agent.pkl")

    envs.close()
    print("ğŸ‰ Training completed!")


if __name__ == "__main__": train()